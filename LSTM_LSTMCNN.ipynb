{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Bidirectional\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data (for now, sell_prices & calendar are not used)\n",
    "\n",
    "data_dir = 'data/'\n",
    "\n",
    "train_sales = pd.read_csv(data_dir + 'sales_train_validation.csv')\n",
    "#sell_prices = pd.read_csv(data_dir + 'sell_prices.csv')\n",
    "#calendar = pd.read_csv(data_dir + 'calendar.csv')\n",
    "submission_file = pd.read_csv(data_dir + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: \n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 95.00 Mb (78.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_sales = reduce_mem_usage(train_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data, for now it only contains the sales and no extra features\n",
    "sales = train_sales.drop([\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"], axis=1).T\n",
    "\n",
    "# normalize\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(sales)\n",
    "sales = scaler.transform(sales)\n",
    "sales = pd.DataFrame(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "\n",
    "timesteps = 28\n",
    "prediction_steps = 1\n",
    "len_window = timesteps + prediction_steps\n",
    "\n",
    "nr_training_days = sales.shape[0]\n",
    "nr_sets = nr_training_days - len_window + 1\n",
    "\n",
    "base, predictions = [], []\n",
    "\n",
    "for i in range(nr_sets):\n",
    "    samples = sales.iloc[i:i+timesteps]\n",
    "    pred = sales.iloc[i+timesteps]\n",
    "    base.append(samples.to_numpy())\n",
    "    predictions.append(pred.to_numpy())\n",
    "    \n",
    "X = np.array(base)\n",
    "y = np.array(predictions)\n",
    "\n",
    "del base, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_scaled_error(y_true,y_pred):\n",
    "    sample_length = 28\n",
    "    forecasting_horizon = 1\n",
    "    upper_bound = sample_length + forecasting_horizon + 1\n",
    "    lower_bound = sample_length + 1 - 1\n",
    "    numerator =  K.sum(K.square(y_true[lower_bound:upper_bound]-y_pred[lower_bound:upper_bound]))\n",
    "    lower_bound = 2 - 1\n",
    "    denominator = (1/(sample_length - 1 )) * K.sum(K.square(y_true[lower_bound:sample_length] - y_true[lower_bound-1:sample_length-1]))\n",
    "    value_to_be_sqrt = (1/forecasting_horizon) * (numerator/denominator)\n",
    "    result = K.sqrt(value_to_be_sqrt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor = 0.1, patience = 2 , verbose = 1, mode= 'min', min_lr = 0.000001)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "n_features = X.shape[2]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, return_sequences=True, input_shape=(timesteps, n_features))))\n",
    "model.add(Bidirectional(LSTM(10)))\n",
    "model.add(Dense(30490))\n",
    "model.compile(optimizer='adam', loss=root_mean_squared_scaled_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1885/1885 [==============================] - 91s 48ms/step - loss: 0.7302\n",
      "Epoch 2/10\n",
      "1885/1885 [==============================] - 80s 43ms/step - loss: 0.7143\n",
      "Epoch 3/10\n",
      "1885/1885 [==============================] - 82s 43ms/step - loss: 0.7135\n",
      "Epoch 4/10\n",
      "1885/1885 [==============================] - 82s 44ms/step - loss: 0.6901\n",
      "Epoch 5/10\n",
      "1885/1885 [==============================] - 83s 44ms/step - loss: 0.6916\n",
      "Epoch 6/10\n",
      "1885/1885 [==============================] - 80s 43ms/step - loss: 0.6850\n",
      "Epoch 7/10\n",
      "1885/1885 [==============================] - 80s 42ms/step - loss: 0.6899\n",
      "Epoch 8/10\n",
      "1885/1885 [==============================] - 80s 42ms/step - loss: 0.6870\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 9/10\n",
      "1885/1885 [==============================] - 79s 42ms/step - loss: 0.6896\n",
      "Epoch 10/10\n",
      "1885/1885 [==============================] - 78s 41ms/step - loss: 0.6739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13ecbea90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "model.fit(X, y, batch_size=32, epochs=10, callbacks=[reduce_lr], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "\n",
    "for i in range(28):    \n",
    "    # get input for prediction by selecting last 28 days from sales\n",
    "    X_pred = []\n",
    "    X_pred.append(sales.iloc[-timesteps:].to_numpy())\n",
    "    X_pred = np.array(X_pred)\n",
    "    \n",
    "    # get prediction\n",
    "    prediction = model.predict(X_pred)\n",
    "    \n",
    "    # add prediction to sales so that it can be used for next prediction\n",
    "    sales.loc[sales.shape[0]] = prediction[0]\n",
    "    \n",
    "predictions = sales.iloc[-28:]\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "predictions = np.round(np.abs(predictions))\n",
    "predictions = pd.DataFrame(predictions).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file\n",
    "predictions_copy = predictions\n",
    "final_submission = pd.concat([predictions, predictions_copy])\n",
    "final_submission.reset_index(drop=True, inplace=True)\n",
    "final_submission = final_submission.astype(int)\n",
    "final_submission.insert(0, 'id', submission_file['id'])\n",
    "final_submission.columns = ['id'] + [f\"F{i}\" for i in range(1, 29)]\n",
    "\n",
    "final_submission.to_csv('submission_msqce_lronplateau.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
