{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M5 forecasting Accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "\n",
    "print(os.listdir(data_dir));\n",
    "\n",
    "\n",
    "sales_train_validation = pd.read_csv(os.path.join(data_dir,'sales_train_validation.csv'))\n",
    "calendar = pd.read_csv(os.path.join(data_dir,'calendar.csv'))\n",
    "sell_prices = pd.read_csv(os.path.join(data_dir,'sell_prices.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales_train_validation.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales_train_validation.iloc[0]['item_id'])\n",
    "print(sales_train_validation.iloc[0]['dept_id'])\n",
    "print(sales_train_validation.iloc[0]['cat_id'])\n",
    "print(sales_train_validation.iloc[0]['store_id'])\n",
    "print(sales_train_validation.iloc[0]['state_id'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales_train_validation.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calendar.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sell_prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = range(1, 1914)\n",
    "time_series_columns = [f'd_{i}' for i in days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = sales_train_validation[time_series_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Approach\n",
    "The data is a bit complex because of all the features that influence the sales as well as the format of the data. What makes it a bit complex in my opinion if that we have products and days. Thus for each product there are 1913 days, it would be simpler if we had one product and 1913 days. Because that problem could be translated to the wheather problem which is used in this tutorial [tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series#the_weather_dataset) where they have days and temperature. So maybe we should think of this as a situation where we have n times , days and temperature, where n is the number of products....\n",
    "\n",
    "Thus I would like to build a simple MLP, where the inputs are the products and sales on different days. Then use an activation function that enables the MLP to \"learn\" the values..\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, 1)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        labels.append(dataset[i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data.iloc[10].plot(subplots=False)\n",
    "#.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"./data\"\n",
    "\n",
    "def get_salesval_coltypes():\n",
    "    keys = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'] + \\\n",
    "        [f\"d_{i}\" for i in range(1, 1914)]\n",
    "    values = ['object', 'category', 'category', 'category', 'category', 'category'] +\\\n",
    "        [\"uint16\" for i in range(1, 1914)]\n",
    "    return dict(zip(keys, values))\n",
    "\n",
    "submission = pd.read_csv(os.path.join(input_path, 'sample_submission.csv'))\n",
    "sales_train_val = pd.read_csv(os.path.join(input_path, 'sales_train_validation.csv'), \n",
    "                              dtype=get_salesval_coltypes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1904</th>\n",
       "      <th>d_1905</th>\n",
       "      <th>d_1906</th>\n",
       "      <th>d_1907</th>\n",
       "      <th>d_1908</th>\n",
       "      <th>d_1909</th>\n",
       "      <th>d_1910</th>\n",
       "      <th>d_1911</th>\n",
       "      <th>d_1912</th>\n",
       "      <th>d_1913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1904  d_1905  d_1906  d_1907  d_1908  \\\n",
       "0       CA    0    0    0    0  ...       1       3       0       1       1   \n",
       "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
       "2       CA    0    0    0    0  ...       2       1       2       1       1   \n",
       "3       CA    0    0    0    0  ...       1       0       5       4       1   \n",
       "4       CA    0    0    0    0  ...       2       1       1       0       1   \n",
       "\n",
       "   d_1909  d_1910  d_1911  d_1912  d_1913  \n",
       "0       1       3       0       1       1  \n",
       "1       1       0       0       0       0  \n",
       "2       1       0       1       1       1  \n",
       "3       0       1       3       7       2  \n",
       "4       1       2       2       2       4  \n",
       "\n",
       "[5 rows x 1919 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_train_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples (30490, 10), preds (30490, 28)\n",
      "Samples (30490, 10), preds (30490, 28)\n",
      "Samples (30490, 10), preds (30490, 28)\n",
      "Samples (30490, 10), preds (30490, 28)\n",
      "Samples (30490, 10), preds (30490, 28)\n",
      "Samples (30490, 10), preds (30490, 28)\n",
      "Samples (30490, 10), preds (30490, 28)\n",
      "Samples (30490, 10), preds (30490, 28)\n",
      "Samples (30490, 10), preds (30490, 28)\n",
      "Samples (30490, 10), preds (30490, 28)\n",
      "Samples (30490, 10), preds (30490, 28)\n",
      "(335390, 10, 1)\n",
      "(335390, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1463"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8# Prepare scalars to normalize data\n",
    "input_scaler = MinMaxScaler()\n",
    "output_scaler = StandardScaler()\n",
    "\n",
    "# Our timeseries data is in cols d_1 to d_1913\n",
    "data = sales_train_val.iloc[:, 6:]\n",
    "#data = (data-data.min())/(data.max()-data.min())\n",
    "\n",
    "# For LSTM, X needs to be a stack of shape (samples, timesteps, features)\n",
    "# So aiming at a shape of  = (~order of 30490 * timesteps, 28, 1)\n",
    "\n",
    "\n",
    "# For later - test train split, for now just get shapes right\n",
    "base = []\n",
    "predictions = []\n",
    "\n",
    "timesteps = 10\n",
    "prediction_steps = 28\n",
    "\n",
    "# Well just iterate through slicing timesteps until we get somewhat near the end. With a\n",
    "# proper train test split, we could be more precise\n",
    "for i in range(1, 12):\n",
    "    samples = data.iloc[:, i*timesteps:i*timesteps+timesteps]\n",
    "    preds = data.iloc[:, i*timesteps+timesteps:i*timesteps+timesteps+prediction_steps]\n",
    "    base.extend(samples.to_numpy())\n",
    "    predictions.extend(preds.to_numpy())\n",
    "    print(f\"Samples {samples.shape}, preds {preds.shape}\")\n",
    "    \n",
    "\n",
    "# Scale and reshape our input\n",
    "X_train = np.array(base)\n",
    "input_scaler.fit(X_train)\n",
    "X_train = input_scaler.transform(X_train)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    \n",
    "\n",
    "# Scale our prediction labels\n",
    "Y_train_orig = np.array(predictions)\n",
    "output_scaler.fit(Y_train_orig)\n",
    "Y_train = output_scaler.transform(Y_train_orig)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "# Note this could be horrible on memory. Later, need to look at generating this in batches\n",
    "del predictions\n",
    "del base\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_scaled_error(y_true,y_pred):\n",
    "    sample_length = 10\n",
    "    forecasting_horizon = 28\n",
    "\n",
    "    numerator = (1/(sample_length-1)) * K.sum(K.square(y_true-y_pred))\n",
    "    denominator = (1/(sample_length - 1 )) * K.sum(K.square(y_true[1:] - y_true[:-1]))\n",
    "    value_to_be_sqrt = (1/forecasting_horizon) * (numerator/denominator)\n",
    "    result = K.square(value_to_be_sqrt)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    print(y_true.shape)\n",
    "    print(y_pred.shape)\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "steps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_steps_out = Y_train.shape[1]\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True, input_shape=(steps, n_features))))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10)))\n",
    "model.add(tf.keras.layers.Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss=root_mean_squared_scaled_error) # this loss needs changing to competition loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 335390 samples\n",
      "Epoch 1/2\n",
      "335390/335390 [==============================] - 199s 594us/sample - loss: 1.4774e-04\n",
      "Epoch 2/2\n",
      "335390/335390 [==============================] - 182s 542us/sample - loss: 1.4513e-04\n",
      "CPU times: user 19min 48s, sys: 3min 7s, total: 22min 56s\n",
      "Wall time: 6min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3a411ea510>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 0.6345 200 56\n",
    "# 0.5633 200 56 4m 14s\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 s, sys: 2.34 s, total: 16.8 s\n",
      "Wall time: 5.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Take a slice of n{timesteps} from the input data\n",
    "x_pred = data.iloc[:,-timesteps:].to_numpy()\n",
    "\n",
    "# Reshape to fit the format for input scalar\n",
    "x_pred = x_pred.reshape((len(sales_train_val), x_pred.shape[1]))\n",
    "# Normalize the input\n",
    "x_pred = input_scaler.transform(x_pred)\n",
    "# Reshape to fit the format for LSTM model\n",
    "x_pred = x_pred.reshape((len(sales_train_val), x_pred.shape[1], 1))\n",
    "\n",
    "# Get our predictions\n",
    "raw_pred = model.predict(x_pred)\n",
    "\n",
    "# Inverse to transform to get the predictions at the right scale\n",
    "all_pred = output_scaler.inverse_transform(raw_pred)\n",
    "# Round the predictions back to integers\n",
    "all_pred = np.round(np.abs(all_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack our predictions into a dataframe\n",
    "validation = pd.concat([pd.DataFrame(all_pred[:,0:prediction_steps]), pd.DataFrame(all_pred[:,-prediction_steps:])])\n",
    "validation = validation.astype(int)\n",
    "\n",
    "# Reset index to match the submission dataframe\n",
    "validation.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Add the id column from the submission dataframe to our results\n",
    "validation['id'] = submission.id\n",
    "validation = validation.reindex(\n",
    "        columns=['id'] + [c for c in validation.columns if c != 'id'], copy=False)\n",
    "\n",
    "# Add the correct colummn names for the submission file format\n",
    "validation.columns = ['id'] + [f\"F{i}\" for i in range(1, 29)]\n",
    "\n",
    "validation.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
